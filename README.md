# PYSPARK-
Implemented a real-world, scenario-based data engineering problem using PySpark and Databricks Delta Lake. The project demonstrates end-to-end data pipeline development including:

🔹Data ingestion from source tables
🔹Deduplication and transformation of records
🔹Handling Slowly Changing Dimensions with Delta Live Tables (DLT)
🔹Ensuring data consistency, reliability, and scalability for analytics

This project simulates challenges faced in industry workflows and showcases best practices in modern data engineering.

📊 Key Features:
✅ Deduplication and handling of late-arriving data
✅ Maintenance of historical records with SCD Type 2
✅ Streaming & batch integration for real-time analytics
✅ Data validation and monitoring for data quality assurance

🛠 Tech Stack:
⚡ PySpark / Spark SQL
💎 Databricks / Delta Lake / DLT
🐍 Python
